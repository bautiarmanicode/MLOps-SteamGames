{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de funciones\n",
    "\n",
    "üì•Importaciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importaci√≥n del m√≥dulo gc para la recolecci√≥n de basura\n",
    "import gc\n",
    "\n",
    "# Importaci√≥n del m√≥dulo pickle para la serializaci√≥n de objetos\n",
    "import pickle\n",
    "\n",
    "# Importaci√≥n del m√≥dulo warnings para controlar advertencias\n",
    "import warnings\n",
    "\n",
    "# Importaci√≥n de pandas y se asigna el alias 'pd' para su uso\n",
    "import pandas as pd\n",
    "\n",
    "# Importaci√≥n de funciones desde data_utils\n",
    "from data_utils import analisis_sentimiento, ej_review_sentimiento\n",
    "\n",
    "\n",
    "\n",
    "# Configuraci√≥n de la omisi√≥n de advertencias\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Carga de la extensi√≥n autoreload y configuraci√≥n para recargar autom√°ticamente m√≥dulos\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "üì¶ Extracci√≥n de los conjuntos de datos\n",
    "* ‚úçÔ∏è **Extraemos df_reviews:** usuarios que realizaron rese√±as de los juegos que consumen.\n",
    "* üéÆ **Extraemos df_games:** juegos disponibles en la plataforma.\n",
    "* üìä **Extraemos df_items:** consumo de los juegos por parte de los usuarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steam_games =   pd.read_parquet('../0 Dataset/steam_games_LISTO.parquet')\n",
    "df_user_items =   pd.read_parquet('../0 Dataset/user_items_LISTO.parquet')\n",
    "df_user_reviews = pd.read_parquet('../0 Dataset/user_review_LISTO.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1 üåê `developer(desarrollador: str)`:\n",
    "\n",
    "Devuelve la cantidad de juegos y porcentaje de contenido Free por a√±o seg√∫n empresa desarrolladora\n",
    "\n",
    " **üìÇ steam_games.json**: developer, release_date, price\n",
    "\n",
    "üåêPara acelerar la velocidad de la api, he decidido crear un DF especifico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_API_developer = df_steam_games[[\"developer\",\"user_id\", \"release_year\", \"price\"]]\n",
    "df_API_developer.to_parquet(\"../0 Dataset/df_API_developer.parquet\")\n",
    "def developer(desarrollador: str):\n",
    "    df_dev = df_API_developer[df_API_developer['developer'] == desarrollador]\n",
    "\n",
    "    grouped = df_dev.groupby('release_year').agg(\n",
    "        items=('user_id', 'count'),  # Cambiar de (x == 0).sum() a 'count' o 'sum'\n",
    "        gratis=('price', lambda x: (x == 0).sum())\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for year, row in grouped.iterrows():\n",
    "        juegos = int(row[\"items\"])\n",
    "        gratis_percent = round(row[\"gratis\"] / juegos * 100, 2) if juegos > 0 else 0\n",
    "        result.append({\n",
    "            \"A√±o\": int(year),\n",
    "            \"Juegos\": juegos,\n",
    "            \"Gratis %\": gratis_percent\n",
    "        })\n",
    "    \n",
    "    # Llamamos al recolector de basura\n",
    "    gc.collect()\n",
    "\n",
    "    return result\n",
    "# Example usage\n",
    "resultado = developer('Valve')\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üåê userdata(User_id: str) : \n",
    "Dinero gastado por el usuario, porcentaje de recomendaci√≥n y cantidad de items.\n",
    "\n",
    "1. üìÇ **user_reviews.json**: user_id, recommend, helpful.total\n",
    "2. üìÇ **users_items.json**: user_id, items_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unir los dataframes en base a la columna \"user_id\"\n",
    "df_userdata = pd.merge(df_user_reviews[['user_id', 'reviews_recommend', 'reviews_helpful']], df_user_items[['user_id', 'items_count']], on='user_id')\n",
    "\n",
    "df_userdata\n",
    "def userdata(user_id):\n",
    "    user = df_API_userdata[df_API_userdata['user_id'] == user_id]\n",
    "    if len(user) > 0:\n",
    "        user_dict = {\n",
    "            'dinero_gastado': user['items'].sum(),\n",
    "            'porcentaje_recomendacion': user['recommend'].mean(),\n",
    "            'cantidad_items': len(user)\n",
    "        }\n",
    "        return user_dict\n",
    "    else:\n",
    "        return None\n",
    "user = userdata('evcentric')\n",
    "if user is not None:\n",
    "    print(user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "_______________________________\n",
    "1 DEF userdata(User_id: str):  \n",
    "\n",
    "devolver cantidad de dinero gastado por el usuario\n",
    "el porcentaje de recomendaci√≥n en base a reviews.recommend \n",
    "y cantidad de items. \n",
    "_______________________________\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def userdata(User_id: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Devuelve la cantidad de dinero gastado por el usuario, el porcentaje de recomendaci√≥n en base a reviews.recommend y la cantidad de items.\n",
    "    \n",
    "    Parameters:\n",
    "    User_id (str): Identificador √∫nico del usuario.\n",
    "    \n",
    "    Returns:\n",
    "    Dict[str, float]: Diccionario con los datos del usuario (dinero gastado, porcentaje de recomendaci√≥n, cantidad de items).\n",
    "    \"\"\"\n",
    "    # Leer los archivos Parquet\n",
    "    user_reviews_df = pd.read_parquet('user_reviews.parquet')\n",
    "    users_items_df = pd.read_parquet('users_items.parquet')\n",
    "    \n",
    "    # Obtener los datos del usuario\n",
    "    user_data = users_items_df[users_items_df['user_id'] == User_id].iloc[0]\n",
    "    \n",
    "    # Obtener las revisiones del usuario\n",
    "    user_reviews = user_reviews_df[user_reviews_df['user_id'] == User_id]\n",
    "    \n",
    "    # Calcular el total de dinero gastado y el porcentaje de recomendaci√≥n\n",
    "    total_spent = user_reviews['helpful.total'].sum()\n",
    "    recommend_count = len(user_reviews[user_reviews['recommend']])\n",
    "    total_reviews = len(user_reviews)\n",
    "    recommend_percentage = (recommend_count / total_reviews) * 100 if total_reviews > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"Dinero gastado\": total_spent,\n",
    "        \"% de recomendaci√≥n\": recommend_percentage,\n",
    "        \"cantidad de items\": user_data['items_count']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2_ üìä An√°lisis de Sentimientos\n",
    "\n",
    "**'sentiment_analysis'** reemplaza a **'reviews_review'** : se crea esta nueva columna para realizar un an√°lisis de sentimientos en los comentarios de los usuarios.\n",
    "\n",
    "**Utilizamos la biblioteca TextBlob** , que es una herramienta de procesamiento de lenguaje natural (NLP) en Python en esta columna y aplico un an√°lisis de sentimiento b√°sico.\n",
    "\n",
    "La escala utilizada es la siguiente:\n",
    "\n",
    "* üëé **0** si el sentimiento es **malo.**\n",
    "* üòê **1** si el sentimiento es **neutral o no hay revisi√≥n.**\n",
    "* üëç **2** si el sentimiento es **positivo.**\n",
    "\n",
    "Esta metodolog√≠a asigna un valor num√©rico a cada texto, en este caso, a los comentarios de los usuarios en relaci√≥n con un juego espec√≠fico, para representar si el sentimiento expresado en el texto es negativo, neutral o positivo.\n",
    "\n",
    "El an√°lisis se basa en la polaridad calculada por TextBlob, donde se considera que las polaridades negativas est√°n por debajo de -0.2, las positivas por encima de 0.2 y las neutrales entre estos valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_reviews['sentiment_analysis'] = df_user_reviews['reviews_review'].apply(analisis_sentimiento)\n",
    "df_user_reviews.head()\n",
    "#Reviso algunos ejemplos para cada una de las clases de sentimiento.\n",
    "ej_review_sentimiento(df_user_reviews['reviews_review'], df_user_reviews['sentiment_analysis'])\n",
    "üóëÔ∏è Al final del proceso, la columna 'reviews_review' se elimina del conjunto de datos. \n",
    "df_user_reviews = df_user_reviews.drop(columns=['reviews_review'])\n",
    "df_user_reviews.columns\n",
    "df_user_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning item x item\n",
    "### def **recomendacion_juego( *`id de producto`* )**:\n",
    "Ingresando el id de producto, deber√≠amos recibir una lista con 5 juegos recomendados similares al ingresado\n",
    "falta similaridad indices .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ML = pd.read_parquet(\"./0 Dataset/F_df_funciones.parquet\")\n",
    "\n",
    "def recomendacion_juego(item_id: int):\n",
    "    with open(\"./0 Dataset/similaridad_indices.pkl\", \"rb\") as archivo:\n",
    "        pickle.load(archivo)\n",
    "    \n",
    "    try:\n",
    "        with open('./0 Dataset/similaridad_indices.pkl', 'rb') as f:\n",
    "            similaridad_indices = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        matrix_sparse = csr_matrix(pd.crosstab(df_ML['item_id'], df_ML['user_id'], values=df_ML['reviews_recommend'], aggfunc='sum').fillna(0))\n",
    "        simil = cosine_similarity(matrix_sparse)\n",
    "        similaridad_indices = {item_id: simil[item_index].argsort()[-6:-1][::-1] for item_index, item_id in enumerate(df_ML['item_id'].unique().tolist())}\n",
    "        with open('./0 Dataset/similaridad_indices.pkl', 'wb') as f:\n",
    "            pickle.dump(similaridad_indices, f)\n",
    "        del matrix_sparse, simil\n",
    "    \n",
    "    try:\n",
    "        item_similar = [df_ML['item_id'].unique().tolist()[i] for i in similaridad_indices[item_id]]\n",
    "        item_similar_nombre = [(df_ML[df_ML['item_id'] == item]['item_id'].values[0], df_ML[df_ML['item_id'] == item]['item_name'].values[0]) for item in item_similar]\n",
    "        return {\"Recomendaciones para Item ID\": item_id, \"Juegos Recomendados\": item_similar_nombre}\n",
    "    except KeyError:\n",
    "        return {\"error\": f\"Item ID {item_id} not found.\"}\n",
    "    finally:\n",
    "        # Liberar memoria utilizando el recolector de basura\n",
    "        gc.collect()\n",
    "\n",
    "recomendacion_juego(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
