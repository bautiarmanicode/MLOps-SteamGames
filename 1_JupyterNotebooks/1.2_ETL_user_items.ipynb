{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **ETL (Extract, Transform, Load)**\n",
    "### **üìÇProcesamos el 2do archivo: `user_items.json.gz`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ **user_reviews.json**:: \n",
    "- üóÉÔ∏è `Items `**:** Desanidado: era una lista de diccionarios\n",
    "- üîÑ `Playtime_forever `: Transformamos los minutos a horas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì¶ **EXTRACT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importamos las librer√≠as que vamos a usar**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas se utiliza para el manejo y an√°lisis de datos tabulares\n",
    "import pyarrow as pa  # PyArrow se utiliza para trabajar con formatos de datos columnares y eficientes como Parquet\n",
    "import pyarrow.parquet as pq  # Importamos Parquet\n",
    "\n",
    "import ast  # AST (Abstract Syntax Trees) se utiliza para interpretar expresiones Python\n",
    "import gzip\n",
    "import json  # JSON se utiliza para trabajar con datos en formato JSON\n",
    "import os  # OS proporciona funciones para interactuar con el sistema operativo\n",
    "import time\n",
    "import warnings  # Warnings se utiliza para gestionar advertencias y filtrarlas si es necesario\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from data_utils import data_type_check, duplicados_columna\n",
    "# Autoreload se utiliza para recargar autom√°ticamente los m√≥dulos al realizar cambios\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Descomprimimos el archivo gz** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descomprimido: ../0 Dataset/users_items.json\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el tiempo de inicio de todo este ipynb \n",
    "start_time = time.time()\n",
    "\n",
    "def descomprimir_archivos_gz(archivos_gz, carpeta_destino):\n",
    "    for archivo_gz in archivos_gz:\n",
    "        with gzip.open(archivo_gz, 'rb') as f_in:\n",
    "            contenido = f_in.read()\n",
    "            archivo_destino = os.path.join(carpeta_destino, os.path.splitext(os.path.basename(archivo_gz))[0])\n",
    "            with open(archivo_destino, 'wb') as f_out:\n",
    "                f_out.write(contenido)\n",
    "        print(f'Archivo descomprimido: {archivo_destino}')\n",
    "\n",
    "# Ejemplo de uso con una lista de archivos gz\n",
    "archivo_gz_a_descomprimir = ['../0 Dataset/users_items.json.gz']\n",
    "carpeta_destino = '../0 Dataset/'\n",
    "\n",
    "descomprimir_archivos_gz(archivo_gz_a_descomprimir, carpeta_destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìÇProcesamos el 2do archivo: `user_items.json.gz`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Tomamos los datos del archivo JSON, transformamos en un DataFrame y realizamos una primera observaci√≥n de su contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>items_count</th>\n",
       "      <th>steam_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60397</th>\n",
       "      <td>76561198063871255</td>\n",
       "      <td>0</td>\n",
       "      <td>76561198063871255</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561198063...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18546</th>\n",
       "      <td>76561197967609413</td>\n",
       "      <td>331</td>\n",
       "      <td>76561197967609413</td>\n",
       "      <td>http://steamcommunity.com/profiles/76561197967...</td>\n",
       "      <td>[{'item_id': '220', 'item_name': 'Half-Life 2'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id  items_count           steam_id  \\\n",
       "60397  76561198063871255            0  76561198063871255   \n",
       "18546  76561197967609413          331  76561197967609413   \n",
       "\n",
       "                                                user_url  \\\n",
       "60397  http://steamcommunity.com/profiles/76561198063...   \n",
       "18546  http://steamcommunity.com/profiles/76561197967...   \n",
       "\n",
       "                                                   items  \n",
       "60397                                                 []  \n",
       "18546  [{'item_id': '220', 'item_name': 'Half-Life 2'...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos una lista vac√≠a llamada \"rows\" donde almacenaremos los datos del archivo JSON.\n",
    "rows = []\n",
    "#Abrir el archivo \"user_reviews.json/australian_user_reviews.json\" con la codificaci√≥n MacRoman.\n",
    "with open(\"../0 Dataset/users_items.json\", encoding='utf-8') as f:\n",
    "    # Leer cada l√≠nea del archivo.\n",
    "    for line in f.readlines():\n",
    "        # Utilizar \"ast.literal_eval\" para convertir cada l√≠nea en un diccionario de Python\n",
    "        # y agregarlo a la lista \"rows\".\n",
    "        rows.append(ast.literal_eval(line))\n",
    "\n",
    "#Crear un DataFrame de Pandas a partir de la lista de diccionarios \"rows\".\n",
    "df_users_items = pd.DataFrame(rows)\n",
    "#Veamos unos registros al asar\n",
    "df_users_items.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la funcion personalizada `data_type_check` invocada desde `data_utils.py` podemos observar:\n",
    "- Variables categ√≥ricas\n",
    "- Variables num√©ricas\n",
    "- Dimensiones del dataframe\n",
    "- Nulos\n",
    "- Tipos de datos\n",
    "- Informacion acerca de los datos faltantes o nulos de cada columna    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " Resumen del dataframe 'df_users_items': \n",
      "\n",
      "========================================\n",
      "Dimensiones:  (88310, 5)\n",
      "       columna  %_no_nulos  %_nulos  total_nulos tipo_dato\n",
      "0      user_id       100.0      0.0            0    object\n",
      "1  items_count       100.0      0.0            0     int64\n",
      "2     steam_id       100.0      0.0            0    object\n",
      "3     user_url       100.0      0.0            0    object\n",
      "4        items       100.0      0.0            0    object\n"
     ]
    }
   ],
   "source": [
    "data_type_check(df_users_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto contiene 5 columnas y 88310 filas sin nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>items_count</th>\n",
       "      <th>steam_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16246</th>\n",
       "      <td>supremosniper</td>\n",
       "      <td>65</td>\n",
       "      <td>76561198089782215</td>\n",
       "      <td>http://steamcommunity.com/id/supremosniper</td>\n",
       "      <td>[{'item_id': '12100', 'item_name': 'Grand Thef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id  items_count           steam_id  \\\n",
       "16246  supremosniper           65  76561198089782215   \n",
       "\n",
       "                                         user_url  \\\n",
       "16246  http://steamcommunity.com/id/supremosniper   \n",
       "\n",
       "                                                   items  \n",
       "16246  [{'item_id': '12100', 'item_name': 'Grand Thef...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users_items.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ **TRANSFORM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Columna \"user_url\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos estas columnas ya que son irrelevantes para el problema que queremos resolver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las columnas irrelevantes que no nos riven\n",
    "df_users_items.drop([\"user_url\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columna **'items'** (es una lista de diccionarios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desanidado de items, que contiene:\n",
    "- Item_name\n",
    "- playtime_forever\n",
    "- playtime_2weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " Resumen del dataframe 'df_users_items': \n",
      "\n",
      "========================================\n",
      "Dimensiones:  (5170015, 8)\n",
      "            columna  %_no_nulos  %_nulos  total_nulos tipo_dato\n",
      "0           user_id      100.00     0.00            0    object\n",
      "1       items_count      100.00     0.00            0     int64\n",
      "2          steam_id      100.00     0.00            0    object\n",
      "3             items       99.67     0.33        16806    object\n",
      "4           item_id       99.67     0.33        16806    object\n",
      "5         item_name       99.67     0.33        16806    object\n",
      "6  playtime_forever       99.67     0.33        16806   float64\n",
      "7   playtime_2weeks       99.67     0.33        16806   float64\n"
     ]
    }
   ],
   "source": [
    "#Creamos una nueva fila para cada elemento de la lista en la columna items\n",
    "df_users_items = df_users_items.explode(\"items\").reset_index()\n",
    "#Eliminamos la columna index\n",
    "df_users_items = df_users_items.drop(columns=\"index\")\n",
    "# Creamos una nueva columna para cada elemento de la lista en la columna items\n",
    "df_users_items = pd.concat([df_users_items, pd.json_normalize(df_users_items['items'])], axis=1)\n",
    "\n",
    "data_type_check(df_users_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playtime_forever: Transformamos los minutos a horas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de limpiar\n",
      "Despues de limpiar\n",
      "\n",
      "========================================\n",
      " Resumen del dataframe 'df_users_items': \n",
      "\n",
      "========================================\n",
      "Dimensiones:  (3302052, 8)\n",
      "            columna  %_no_nulos  %_nulos  total_nulos tipo_dato\n",
      "0           user_id      100.00     0.00            0    object\n",
      "1       items_count      100.00     0.00            0     int64\n",
      "2          steam_id      100.00     0.00            0    object\n",
      "3             items       99.49     0.51        16806    object\n",
      "4           item_id       99.49     0.51        16806    object\n",
      "5         item_name       99.49     0.51        16806    object\n",
      "6  playtime_forever       99.49     0.51        16806   float64\n",
      "7   playtime_2weeks       99.49     0.51        16806   float64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "playtime_forever\n",
    "'''\n",
    "\n",
    "# mostramos un playtime_forever de ejemplo\n",
    "print(\"Antes de limpiar\")\n",
    "df_users_items.head(1)\n",
    "\n",
    "#borramos donde no haya tiempo de juego\n",
    "df_users_items = df_users_items[df_users_items['playtime_forever'] != 0]\n",
    "#Convertimos los minutos a horas\n",
    "df_users_items['playtime_forever'] = df_users_items['playtime_forever'] / 60\n",
    "\n",
    "# mostramos un playtime_forever de ejemplo\n",
    "print(\"Despues de limpiar\")\n",
    "df_users_items.head(1)\n",
    "data_type_check(df_users_items)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eliminar espacios en blanco al principio y al final de los valores\n",
    "df_users_items['user_id'] = df_users_items['user_id'].str.strip()\n",
    "# Llenar valores nulos con un valor espec√≠fico (por ejemplo, cadena vac√≠a)\n",
    "df_users_items['user_id'].fillna(' ', inplace=True)\n",
    "# Eliminar caracteres especiales o no imprimibles\n",
    "df_users_items['user_id'] = df_users_items['user_id'].str.replace(r'\\W', '')\n",
    "# Convertir a tipo string\n",
    "df_users_items = df_users_items.astype({'user_id': 'string'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **item_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_items['item_id'] = df_users_items['item_id'].fillna(-1)\n",
    "# Remove empty strings (if needed)\n",
    "df_users_items['item_id'] = df_users_items['item_id'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " Resumen del dataframe 'df_users_items': \n",
      "\n",
      "========================================\n",
      "Dimensiones:  (3302052, 8)\n",
      "            columna  %_no_nulos  %_nulos  total_nulos       tipo_dato\n",
      "0           user_id      100.00     0.00            0  string[python]\n",
      "1       items_count      100.00     0.00            0           int64\n",
      "2          steam_id      100.00     0.00            0          object\n",
      "3             items       99.49     0.51        16806          object\n",
      "4           item_id       99.49     0.51        16806          object\n",
      "5         item_name       99.49     0.51        16806          object\n",
      "6  playtime_forever       99.49     0.51        16806         float64\n",
      "7   playtime_2weeks       99.49     0.51        16806         float64\n"
     ]
    }
   ],
   "source": [
    "# Call your data_type_check function here\n",
    "data_type_check(df_users_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì§ **LOAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '0 Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#convertimos a csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m items\u001b[38;5;241m=\u001b[39m \u001b[43mdf_users_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./0 Dataset/user_items_LIMPIO.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Leemos el archivo CSV en un DataFrame de pandas\u001b[39;00m\n\u001b[0;32m      4\u001b[0m items \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./0 Dataset/user_items_LIMPIO.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\El Bauto\\Desktop\\HENRY GITHUB\\MACHINE LEARNING OPS\\MachineLearning\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\El Bauto\\Desktop\\HENRY GITHUB\\MACHINE LEARNING OPS\\MachineLearning\\venv\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El Bauto\\Desktop\\HENRY GITHUB\\MACHINE LEARNING OPS\\MachineLearning\\venv\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\El Bauto\\Desktop\\HENRY GITHUB\\MACHINE LEARNING OPS\\MachineLearning\\venv\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\El Bauto\\Desktop\\HENRY GITHUB\\MACHINE LEARNING OPS\\MachineLearning\\venv\\lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\El Bauto\\Desktop\\HENRY GITHUB\\MACHINE LEARNING OPS\\MachineLearning\\venv\\lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '0 Dataset'"
     ]
    }
   ],
   "source": [
    "#convertimos a csv\n",
    "items= df_users_items.to_csv('./0 Dataset/user_items_LIMPIO.csv',index=False) \n",
    "# Leemos el archivo CSV en un DataFrame de pandas\n",
    "items = pd.read_csv('./0 Dataset/user_items_LIMPIO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tengo esto dos veces+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de pandas a una tabla de PyArrow\n",
    "table = pa.Table.from_pandas(items)\n",
    "# Escribir la tabla en un archivo Parquet\n",
    "pq.write_table(table, './0 Dataset/user_items_LISTO.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " Resumen del Dataframe \n",
      "========================================\n",
      "Dimensiones:  (3285246, 7)\n",
      "            columna  %_no_nulos  %_nulos  total_nulos       tipo_dato\n",
      "0           user_id       100.0      0.0            0  string[python]\n",
      "1       items_count       100.0      0.0            0           int64\n",
      "2          steam_id       100.0      0.0            0          object\n",
      "3           item_id       100.0      0.0            0           int32\n",
      "4         item_name       100.0      0.0            0          object\n",
      "5  playtime_forever       100.0      0.0            0         float64\n",
      "6   playtime_2weeks       100.0      0.0            0         float64\n"
     ]
    }
   ],
   "source": [
    "# Convertir la tabla de PyArrow a un DataFrame de pandas\n",
    "items_parquet = table.to_pandas()\n",
    "#Borramos el csv limpio ya que nos sirve mas en parquet\n",
    "os.remove('./0 Dataset/user_items_LIMPIO.csv')\n",
    "#Dejamos informacion de nuestro dataset\n",
    "data_type_check(df_users_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì§ **LOAD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos el archivo descomprimidos que ahora tenemos limpio y liviano en formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo eliminado: ./0 Dataset/steam_games.json\n",
      "Archivo eliminado: ./0 Dataset/user_reviews.json\n",
      "Archivo eliminado: ./0 Dataset/users_items.json\n"
     ]
    }
   ],
   "source": [
    "# Lista de archivos descomprimidos\n",
    "gz_descomprimidos = [\n",
    "    './0 Dataset/users_items.json'\n",
    "]\n",
    "\n",
    "# Eliminar archivos descomprimidos\n",
    "for archivo_json in gz_descomprimidos:\n",
    "    try:\n",
    "        os.remove(archivo_json)\n",
    "        print(f'Archivo eliminado: {archivo_json}')\n",
    "    except FileNotFoundError:\n",
    "        print(f'Archivo no encontrado: {archivo_json}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos el tiempo de ejecucion total de nuestro proceso ETL üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo total de ejecuci√≥n de este ipynb: 3.79 minutos\n"
     ]
    }
   ],
   "source": [
    "# Obtener el tiempo de finalizaci√≥n\n",
    "end_time = time.time()\n",
    "# Calcular el tiempo total de ejecuci√≥n\n",
    "total_time = end_time - start_time\n",
    "# Convertir a minutos y redondear a 2 decimales\n",
    "total_time_minutes = round(total_time / 60, 2)\n",
    "# Imprimir resultados\n",
    "print(f\"Tiempo total de ejecuci√≥n de este ipynb: {total_time_minutes} minutos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
